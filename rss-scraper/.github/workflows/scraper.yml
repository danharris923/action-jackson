name: RSS Scraper Automation

on:
  # Run every hour at 15 minutes past the hour to avoid peak load times
  schedule:
    - cron: '15 * * * *'
  
  # Allow manual triggering for testing and immediate updates
  workflow_dispatch:
    inputs:
      use_playwright:
        description: 'Enable Playwright for JavaScript-heavy pages'
        required: false
        default: 'false'
        type: choice
        options:
          - 'true'
          - 'false'
      max_retries:
        description: 'Maximum retry attempts for failed requests'
        required: false
        default: '3'
        type: string

  # Run on pushes to main branch for testing
  push:
    branches: [ main ]
    paths: 
      - 'scraper/**'
      - '.github/workflows/scraper.yml'
      - 'requirements.txt'

jobs:
  scrape-feeds:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        # Fetch full history for better caching
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
        cache-dependency-path: 'requirements.txt'

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Install Playwright browsers
      run: |
        playwright install --with-deps chromium
      if: github.event.inputs.use_playwright == 'true' || env.USE_PLAYWRIGHT == 'true'

    - name: Create data directory
      run: |
        mkdir -p data

    - name: Configure scraper environment
      run: |
        cat > .env << EOF
        RSS_SOURCES=${{ secrets.RSS_SOURCES }}
        AMAZON_TAG_US=${{ secrets.AMAZON_TAG_US }}
        AMAZON_TAG_CA=${{ secrets.AMAZON_TAG_CA }}
        USE_PLAYWRIGHT=${{ github.event.inputs.use_playwright || 'false' }}
        MAX_RETRIES=${{ github.event.inputs.max_retries || '3' }}
        OUTPUT_JSON=data/deals.json
        ENVIRONMENT=production
        EOF

    - name: Run RSS scraper
      id: scraper
      run: |
        cd rss-scraper
        echo "Starting RSS scraper at $(date)"
        
        # Run scraper with timeout
        timeout 25m python -m scraper.main || {
          exit_code=$?
          if [ $exit_code -eq 124 ]; then
            echo "::error::Scraper timed out after 25 minutes"
            exit 1
          else
            echo "::error::Scraper failed with exit code $exit_code"
            exit $exit_code
          fi
        }
        
        echo "Scraper completed at $(date)"
        
        # Check if output file was created
        if [ ! -f data/deals.json ]; then
          echo "::error::Output file data/deals.json was not created"
          exit 1
        fi
        
        # Validate JSON format
        if ! python -m json.tool data/deals.json > /dev/null; then
          echo "::error::Generated JSON file is invalid"
          exit 1
        fi
        
        # Get statistics
        deal_count=$(python -c "import json; data=json.load(open('data/deals.json')); print(len(data))")
        affiliate_count=$(python -c "import json; data=json.load(open('data/deals.json')); print(sum(len([l for l in item.get('processed_links', []) if l.get('is_affiliate', False)]) for item in data))")
        
        echo "deals_count=$deal_count" >> $GITHUB_OUTPUT
        echo "affiliate_links_count=$affiliate_count" >> $GITHUB_OUTPUT
        
        echo "✅ Successfully scraped $deal_count deals with $affiliate_count affiliate links"

    - name: Validate output file
      run: |
        cd rss-scraper
        
        # Check file size (should be reasonable)
        file_size=$(stat -c%s "data/deals.json")
        echo "Output file size: ${file_size} bytes"
        
        if [ $file_size -gt 10485760 ]; then  # 10MB limit
          echo "::warning::Output file is very large (${file_size} bytes)"
        fi
        
        if [ $file_size -lt 10 ]; then  # Minimum size check
          echo "::error::Output file is suspiciously small (${file_size} bytes)"
          exit 1
        fi

    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      with:
        name: deals-data-${{ github.run_id }}
        path: rss-scraper/data/deals.json
        retention-days: 7
      if: always()

    - name: Deploy to Vercel
      uses: amondnet/vercel-action@v25
      with:
        vercel-token: ${{ secrets.VERCEL_TOKEN }}
        vercel-org-id: ${{ secrets.VERCEL_ORG_ID }}
        vercel-project-id: ${{ secrets.VERCEL_PROJECT_ID }}
        working-directory: rss-scraper
        vercel-args: '--prod'
      if: success()

    - name: Notify on success
      if: success()
      run: |
        echo "🎉 RSS scraper completed successfully!"
        echo "📊 Scraped ${{ steps.scraper.outputs.deals_count }} deals"
        echo "🔗 Found ${{ steps.scraper.outputs.affiliate_links_count }} affiliate links"
        echo "🚀 Deployed to Vercel"

    - name: Notify on failure
      if: failure()
      run: |
        echo "❌ RSS scraper failed!"
        echo "::error::Check the logs above for details"
        
        # Upload debug info on failure
        if [ -f rss-scraper/data/deals.json ]; then
          echo "Partial output file was created:"
          ls -la rss-scraper/data/deals.json
          head -n 20 rss-scraper/data/deals.json || true
        fi

  # Optional: Run tests before scraping in production
  test:
    runs-on: ubuntu-latest
    if: github.event_name == 'push'
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install dependencies
      run: |
        cd rss-scraper
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run tests
      run: |
        cd rss-scraper
        python -m pytest tests/ -v --cov=scraper --cov-report=term-missing

    - name: Run linting
      run: |
        cd rss-scraper
        ruff check scraper/ tests/
        mypy scraper/

  # Cleanup old artifacts to save storage
  cleanup:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    needs: scrape-feeds
    
    steps:
    - name: Delete old artifacts
      uses: actions/github-script@v7
      with:
        script: |
          const artifacts = await github.rest.actions.listArtifactsForRepo({
            owner: context.repo.owner,
            repo: context.repo.repo,
            per_page: 100
          });
          
          const oldArtifacts = artifacts.data.artifacts.filter(artifact => {
            const daysOld = (Date.now() - new Date(artifact.created_at)) / (1000 * 60 * 60 * 24);
            return daysOld > 2 && artifact.name.startsWith('deals-data-');
          });
          
          for (const artifact of oldArtifacts) {
            console.log(`Deleting artifact: ${artifact.name}`);
            await github.rest.actions.deleteArtifact({
              owner: context.repo.owner,
              repo: context.repo.repo,
              artifact_id: artifact.id
            });
          }
          
          console.log(`Cleaned up ${oldArtifacts.length} old artifacts`);